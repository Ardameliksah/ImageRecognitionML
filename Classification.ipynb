{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 28, 28)\n",
      "(5000, 28, 28)\n",
      "(28, 28)\n",
      "[1 3 3 ... 2 3 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_images = np.load('quickdraw_subset_np/train_images.npy')\n",
    "train_labels = np.load('quickdraw_subset_np/train_labels.npy')\n",
    "test_images = np.load('quickdraw_subset_np/test_images.npy')\n",
    "test_labels = np.load('quickdraw_subset_np/test_labels.npy')\n",
    "\n",
    "print(train_images.shape) # (20000, 28, 28)\n",
    "print(test_images.shape) # (5000, 28, 28)\n",
    "print(train_images[0].shape) # 28 x 28\n",
    "print(train_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1907fd0a510>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(train_images[1234])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Feature Extraction\n",
    "\n",
    "train_flat = train_images.reshape(train_images.shape[0], -1)\n",
    "test_flat = test_images.reshape(test_images.shape[0], -1)\n",
    "\n",
    "# PCA\n",
    "#https://medium.com/technological-singularity/build-a-principal-component-analysis-pca-algorithm-from-scratch-7515595bf08b\n",
    "def PCA_from_Scratch(X, n_components):\n",
    "    n_rows = X.shape[0]\n",
    "    X = (X - np.mean(X,axis=0))/(np.std(X,axis=0)+1e-4)\n",
    "    cov_mat = sum([X[i].reshape(-1, 1) @ X[i].reshape(1, -1) for i in range(n_rows)]) / n_rows\n",
    "    eigen_values, eigen_vectors  = np.linalg.eig(cov_mat)\n",
    "    sort = np.argsort(eigen_values)[::-1]\n",
    "    principal_components = eigen_vectors[:,sort]\n",
    "    return principal_components[:n_components]\n",
    "\n",
    "def transform(X,principal_components):\n",
    "  X = X.copy()\n",
    "  X_proj = X.dot(principal_components.T)\n",
    "  return X_proj\n",
    "\n",
    "\n",
    "# LDA \n",
    "#https://www.kaggle.com/code/egazakharenko/linear-discriminant-analysis-lda-from-scratch\n",
    "class LDA():\n",
    "  def __init__(self, n_components=None):\n",
    "     self.n_components = n_components\n",
    "  def fit(self,X,y):\n",
    "     self.X = X\n",
    "     self.y = y\n",
    "     samples = X.shape[0]\n",
    "     features= X.shape[1]\n",
    "     classes, cls_counts = np.unique(y,return_counts=True)\n",
    "     priors = cls_counts/samples\n",
    "     X_mean = np.array([X[y==cls].mean(axis=0) for cls in classes])\n",
    "     betweenCLSdeviation = X_mean - X.mean(axis=0)\n",
    "     withinCLSdeviation = X - X_mean[y]\n",
    "\n",
    "     Sb = priors* betweenCLSdeviation.T @ betweenCLSdeviation\n",
    "     Sw = withinCLSdeviation.T @ withinCLSdeviation / samples\n",
    "     Sw_inv = np.linalg.pinv(Sw)\n",
    "     eigvals, eigvecs = np.linalg.eig(Sw_inv @ Sb)\n",
    "     self.dvecs = eigvecs[:, np.argsort(eigvals)[::-1]]\n",
    "     self.weights = X_mean @ self.dvecs @ self.dvecs.T\n",
    "     self.bias = np.log(priors) - 0.5 * np.diag(X_mean @ self.weights.T)\n",
    "     if self.n_components is None:\n",
    "        self.n_components = min(classes.size - 1, features)\n",
    "  def transform(self, X):\n",
    "    return X @ self.dvecs[:, : self.n_components]\n",
    "\n",
    "  def predict(self, X_test):\n",
    "    scores = X_test @ self.weights.T + self.bias\n",
    "    return np.argmax(scores, axis=1)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs = PCA_from_Scratch(train_flat, 2)\n",
    "train_proj = transform(train_flat, pcs)\n",
    "train_proj.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_normal = train_flat/255\n",
    "test_normal = test_flat/255\n",
    "train_normal.shape\n",
    "lda = LDA()\n",
    "lda.fit(train_normal,train_labels)\n",
    "lda_train = lda.transform(train_normal)\n",
    "lda_test = lda.transform(test_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myKNearesNeighbour():\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "    def fit(self, X_train,y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "    def predict(self, X_test):\n",
    "        self.X_test = X_test\n",
    "        y_pred = np.zeros(self.X_test.shape[0])\n",
    "        for i, image in enumerate(X_test):\n",
    "            neighbours = self.findNeighbours(image)\n",
    "            y_pred[i] = self.mode(neighbours)\n",
    "        return y_pred\n",
    "    def findNeighbours(self, image):\n",
    "        sortedList = np.argsort(self.Euclidean(image)) \n",
    "        y_train_sorted = self.y_train[sortedList]\n",
    "        return y_train_sorted[:self.k]\n",
    "    def mode(self, vector):\n",
    "        values, counts = np.unique(vector, return_counts=True)\n",
    "        return values[np.argmax(counts)]\n",
    "    def Euclidean(self,image):\n",
    "        distances = np.sqrt(np.sum((self.X_train - image) ** 2, axis=1))\n",
    "        return distances\n",
    "    \n",
    "        \n",
    "\n",
    "def score(y_pred, y_true):\n",
    "        accuracy = np.mean(y_pred == y_true)\n",
    "        return accuracy\n",
    "train_normal = train_flat/255\n",
    "test_normal = test_flat/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7192\n",
      "0.734\n",
      "0.7424\n",
      "0.7474\n"
     ]
    }
   ],
   "source": [
    "train_normal = train_flat/255\n",
    "test_normal = test_flat/255\n",
    "\n",
    "n_neighbours = [3,5,7,9]\n",
    "scores = []\n",
    "for k in n_neighbours:\n",
    "    KNN = myKNearesNeighbour(k)\n",
    "    KNN.fit(lda_train,train_labels)\n",
    "    y_pred = KNN.predict(lda_test)\n",
    "    result = score(y_pred, test_labels)\n",
    "    scores.append(result)\n",
    "    print(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.score(y_pred, y_true)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One is done\n",
      "One is done\n",
      "One is done\n",
      "One is done\n",
      "One is done\n"
     ]
    }
   ],
   "source": [
    "componentAmount = [16, 32, 64, 128, 256]\n",
    "principal_comps = []\n",
    "train_transform = []\n",
    "test_transform = []\n",
    "y_pred = []\n",
    "scorePCA = []\n",
    "\n",
    "for number in componentAmount:\n",
    "    pcs = PCA_from_Scratch(train_flat, number)\n",
    "    principal_comps.append(pcs)\n",
    "    \n",
    "    train_proj = transform(train_flat, pcs)\n",
    "    test_proj = transform(test_flat, pcs)\n",
    "    train_transform.append(train_proj)\n",
    "    test_transform.append(test_proj)\n",
    "    \n",
    "    KNN_PCA = myKNearesNeighbour(3)\n",
    "    KNN_PCA.fit(train_proj, train_labels)\n",
    "    pred = KNN_PCA.predict(test_proj)\n",
    "    y_pred.append(pred)\n",
    "    \n",
    "    scorePCA.append(score(pred, test_labels))\n",
    "    print(\"One is done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.44, 0.549, 0.6848, 0.7766, 0.8148]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorePCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myNaiveBayes():\n",
    "    def __init__(self):\n",
    "        self.priors = {}\n",
    "        self.likelihoods = {}\n",
    "    def fit(self,X_train,y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.classes = np.unique(y_train)\n",
    "        for w in self.classes:\n",
    "            X_train_w = X_train[y_train == w]\n",
    "            P_w = X_train_w.shape[0] / X_train.shape[0] # Prior Densities P(w_j)\n",
    "            self.priors[w] = P_w\n",
    "            self.likelihoods[w] = {}\n",
    "            for x in range(X_train.shape[1]):\n",
    "                feature_x = X_train_w[:,x]\n",
    "                mean_x_w = np.mean(feature_x)\n",
    "                var_x_w = np.var(feature_x)\n",
    "                self.likelihoods[w][x] = (mean_x_w, var_x_w)\n",
    "\n",
    "    def gaussianDensity(self,x,mean,var):\n",
    "        denu = 1.0 / np.sqrt(2.0 * np.pi * (var + 1e-4))\n",
    "        inexp = -((x - mean) ** 2) / (2 * (var + 1e-4))\n",
    "        return denu*np.exp(inexp)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        y_pred = []\n",
    "        for x in X_test:\n",
    "            posteriors = {}\n",
    "            for c in self.classes:\n",
    "                log_prior = np.log(self.priors[c])\n",
    "                log_likelihood = 0\n",
    "                for i in range(len(x)):\n",
    "                    mean, var = self.likelihoods[c][i]\n",
    "                    log_likelihood += np.log(self.gaussianDensity(x[i], mean, var))\n",
    "                posteriors[c] = log_prior+log_likelihood\n",
    "            y_pred.append(max(posteriors, key=posteriors.get))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_9304\\2853722943.py:34: RuntimeWarning: divide by zero encountered in log\n",
      "  log_likelihood += np.log(self.gaussianDensity(x[i], mean, var))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5674"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mybayes = myNaiveBayes()\n",
    "mybayes.fit(train_normal,train_labels)\n",
    "y_pred = mybayes.predict(test_normal)\n",
    "score(y_pred,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class myMultiClassLogisticRegression():\n",
    "    def __init__(self, learning_rate=0.1, maxiterations=10000, mu=0.0):\n",
    "        \"\"\"\n",
    "        :param learning_rate: Step size for gradient descent (eta).\n",
    "        :param maxiterations: Number of iterations to run gradient descent.\n",
    "        :param mu: L2 regularization coefficient.\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.maxiterations = maxiterations\n",
    "        self.mu = mu  # regularization strength\n",
    "        self.W = None  # weight matrix\n",
    "\n",
    "    def OneHotEncoder(self, y_train, num_classes):\n",
    "        \"\"\"\n",
    "        Convert integer labels (0..num_classes-1) into one-hot vectors.\n",
    "        :param y_train: shape (N,)\n",
    "        :param num_classes: total number of classes\n",
    "        :return: one-hot array of shape (N, num_classes)\n",
    "        \"\"\"\n",
    "        y_trainNew = np.zeros((y_train.shape[0], num_classes))\n",
    "        for i, label in enumerate(y_train):\n",
    "            y_trainNew[i, label] = 1\n",
    "        return y_trainNew\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        \"\"\"\n",
    "        Numerically stable softmax.\n",
    "        :param Z: shape (N, C) - the logits for each class\n",
    "        :return: shape (N, C) - the softmax probabilities\n",
    "        \"\"\"\n",
    "        Z_shifted = Z - np.max(Z, axis=1, keepdims=True)  # stability\n",
    "        exp_Z = np.exp(Z_shifted)\n",
    "        return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
    "\n",
    "    def loss(self, X, Y, W):\n",
    "        \n",
    "        N = X.shape[0]\n",
    "        logits = X @ W  # shape (N, C)\n",
    "        P = self.softmax(logits)  # shape (N, C)\n",
    "        eps = 1e-15\n",
    "        cross_entropy = -np.mean(np.sum(Y * np.log(P + eps), axis=1))\n",
    "        # L2 regularization term\n",
    "        reg_term = self.mu * np.sum(W**2)\n",
    "        return cross_entropy + reg_term\n",
    "\n",
    "    def gradient(self, X, Y, W):\n",
    "        \n",
    "        N = X.shape[0]\n",
    "        logits = X @ W  # shape (N, C)\n",
    "        P = self.softmax(logits)  # shape (N, C)\n",
    "        grad = (1.0 / N) * (X.T @ (P - Y)) + 2.0 * self.mu * W\n",
    "        return grad\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "       \n",
    "        self.n_images, self.n_features = X_train.shape\n",
    "        num_classes = len(np.unique(y_train))\n",
    "\n",
    "        # One-hot encode the labels\n",
    "        Y_onehot = self.OneHotEncoder(y_train, num_classes)\n",
    "\n",
    "        # Initialize weights (d, C)\n",
    "        self.W = np.zeros((self.n_features, num_classes))\n",
    "\n",
    "        # Run gradient descent\n",
    "        for _ in range(self.maxiterations):\n",
    "            grad = self.gradient(X_train, Y_onehot, self.W)\n",
    "            self.W -= self.learning_rate * grad\n",
    "\n",
    "    def predict(self, X_test):\n",
    "       \n",
    "        logits = X_test @ self.W  # shape (M, C)\n",
    "        P = self.softmax(logits)  # shape (M, C)\n",
    "        return np.argmax(P, axis=1)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7508"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myLog = myMultiClassLogisticRegression()\n",
    "myLog.fit(train_normal,train_labels)\n",
    "y_pred = myLog.predict(test_normal)\n",
    "score(y_pred,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class OneVsAllLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.1, max_iter=1000, _lambda=0.1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self._lambda = _lambda\n",
    "        self.classifiers = None\n",
    "        self.classes_ = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1.0 / (1 + np.exp(-z))\n",
    "\n",
    "    def cost_fun(self, W, X, y):\n",
    "        N  = X.shape[0]\n",
    "        h = self.sigmoid(X @ W)\n",
    "        eps = 1e-15\n",
    "        cost = -(1.0/N) * (y.T @ np.log(h + eps) + (1 - y).T @ np.log(1 - h + eps)) + (self._lambda / (2*N)) * np.sum(W**2)\n",
    "        return cost\n",
    "\n",
    "    def gradient(self, W, X, y):\n",
    "        N = X.shape[0]\n",
    "        h = self.sigmoid(X @ W)\n",
    "        grad = (1.0/N) * X.T @ (h - y) + (self._lambda/N) * W\n",
    "        return grad\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        self.classes_ = np.unique(y)\n",
    "        k = len(self.classes_)\n",
    "        all_theta = np.zeros((k, n))\n",
    "        for i, cls in enumerate(self.classes_):\n",
    "            binary_y = (y == cls).astype(float)\n",
    "            theta = np.zeros(n)\n",
    "            for _ in range(self.max_iter):\n",
    "                theta -= self.learning_rate * self.gradient(theta, X, binary_y)\n",
    "            all_theta[i] = theta\n",
    "        self.classifiers = all_theta\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.sigmoid(X.dot(self.classifiers.T))\n",
    "        predictions = np.array([self.classes_[np.argmax(probs[i])] for i in range(X.shape[0])])\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "macroF1() got an unexpected keyword argument 'y_test'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m modelOA\u001b[38;5;241m.\u001b[39mpredict(test_normal)\n\u001b[0;32m     29\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m score(y_pred\u001b[38;5;241m=\u001b[39my_pred,y_test\u001b[38;5;241m=\u001b[39mtest_labels)\n\u001b[1;32m---> 30\u001b[0m macro \u001b[38;5;241m=\u001b[39m macroF1(y_pred\u001b[38;5;241m=\u001b[39my_pred,y_test\u001b[38;5;241m=\u001b[39mtest_labels)            \n\u001b[0;32m     31\u001b[0m results\u001b[38;5;241m.\u001b[39mappend((lr, accuracy))\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnevsAll Learning Rate:\u001b[39m\u001b[38;5;124m\"\u001b[39m, lr, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLambda:\u001b[39m\u001b[38;5;124m\"\u001b[39m, lamb, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy)\n",
      "\u001b[1;31mTypeError\u001b[0m: macroF1() got an unexpected keyword argument 'y_test'"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.01, 0.1, 0.5, 1.0]\n",
    "lambas = [0, 0.01, 0.05, 0.1, 0.5, 1]\n",
    "results = []\n",
    "accuracy = []\n",
    "\n",
    "def macroF1(y_pred, y_true):\n",
    "    labels = np.unique(y_true)\n",
    "    f1_scores = []\n",
    "\n",
    "    for label in labels:\n",
    "        tp = np.sum((y_pred == label) & (y_true == label))\n",
    "        fp = np.sum((y_pred == label) & (y_true != label))\n",
    "        fn = np.sum((y_pred != label) & (y_true == label))\n",
    "\n",
    "        precision = tp / (tp + fp + 1e-8)\n",
    "        recall = tp / (tp + fn + 1e-8)\n",
    "        \n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for lamb in lambas:\n",
    "        modelOA = OneVsAllLogisticRegression(learning_rate=lr, max_iter=1000, _lambda=lamb)\n",
    "        \n",
    "        modelMC = myMultiClassLogisticRegression(learning_rate=lr, maxiterations=1000, mu=lr)\n",
    "        \n",
    "        modelOA.fit(train_normal, train_labels)\n",
    "        y_pred = modelOA.predict(test_normal)\n",
    "        accuracy = score(y_pred=y_pred,y_true=test_labels)\n",
    "        macro = macroF1(y_pred=y_pred,y_true=test_labels)            \n",
    "        results.append((lr, accuracy))\n",
    "        print(\"OnevsAll Learning Rate:\", lr, \"Lambda:\", lamb, \"Test Accuracy:\", accuracy)\n",
    "\n",
    "        modelMC.fit(train_normal, train_labels)\n",
    "        y_pred = modelMC.predict(test_normal)\n",
    "        accuracy = score(y_pred=y_pred,y_true=test_labels)\n",
    "        macro = macroF1(y_pred=y_pred,y_true=test_labels)            \n",
    "        results.append((lr, accuracy))\n",
    "        print(\"MultiClass Learning Rate:\", lr, \"Lambda:\", lamb, \"Test Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
